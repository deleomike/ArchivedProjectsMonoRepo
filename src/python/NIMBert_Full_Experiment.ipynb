{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef06c8b1-a3ab-4471-ab28-61239c534e91",
   "metadata": {},
   "source": [
    "# NIM BERT Full Experiment\n",
    "\n",
    "Michael DeLeo\n",
    "\n",
    "Dr. Erhan Guven\n",
    "\n",
    "1. Imports\n",
    "2. Config\n",
    "3. NIM Game Players\n",
    "    - GURU\n",
    "    - QLearner\n",
    "    - RAndom\n",
    "4. Generate NIM Games\n",
    "5. Train the Tokenizer\n",
    "6. Build the Datasets\n",
    "7. Build the BERT Model\n",
    "8. Train the BERT Models\n",
    "9. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235bf249-b9d9-44e4-816b-7396ab110eaa",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6660ea2b-1135-4660-8987-67b6175fa45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig, BertForMaskedLM, TrainingArguments, Trainer, \n",
    "    DataCollatorForLanguageModeling, BertTokenizer\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from pathlib import Path\n",
    "from random import randint, choice, random\n",
    "from re import search\n",
    "from tqdm.notebook import tqdm\n",
    "from BertHarmon import BertHarmon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800d99ba-a2d9-4421-bfe5-873c1e952184",
   "metadata": {},
   "source": [
    "## 2. Config Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "352067ab-5403-494d-a7f2-faa26b657a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "NUM_PROC = 20 # Change to 1 if on windows\n",
    "\n",
    "if NUM_PROC >  1:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "    BATCHED = True\n",
    "else:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    BATCHED = False\n",
    "\n",
    "# Tokenizer\n",
    "MAX_VOCAB = 1000\n",
    "TOKENIZER_DIR = os.path.abspath(\"./nim-tokenizer\")\n",
    "TOKENIZER_PATH = os.path.join(TOKENIZER_DIR, \"vocab.txt\")\n",
    "MAX_STR_LEN = 12\n",
    "\n",
    "# NIM\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Model\n",
    "NUM_HIDDEN_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c356123b-b92b-4995-8f5b-16c06c61071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(TOKENIZER_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ecd3c4-2c12-4463-83ae-15c2e2ce746e",
   "metadata": {},
   "source": [
    "## 3. NIM Game Players"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d39b2b0-abf0-42b0-8ada-650b896546f5",
   "metadata": {},
   "source": [
    "### Game Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef170de7-323b-4472-b017-3adae36234d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize starting position\n",
    "def init_game():\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818215ee-b030-4274-94b2-68e8564a649f",
   "metadata": {},
   "source": [
    "### Init of player functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb5c0430-069a-4e83-965f-5895b89bf807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Nim player\n",
    "def nim_random(_st, random_chance=None):\n",
    "    \"\"\"\n",
    "    random_chance is a dummy param here. does not do anything\n",
    "    \"\"\"\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af221cea-632f-4755-a0df-09bcb9e6537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(st, random_chance=None):\n",
    "    \"\"\"\n",
    "    st: game state [a, b, c]\n",
    "    random_chance: if none, then just play as player. If some number [0, 1], then\n",
    "        this is the probability of making a random move instead\n",
    "    \"\"\"\n",
    "    if random_chance is not None:\n",
    "        if random() <= random_chance:\n",
    "            return nim_random(st)\n",
    "    \n",
    "    xored = st[0] ^ st[1] ^ st[2]\n",
    "    \n",
    "    if xored == 0:\n",
    "        return nim_random(st)\n",
    "    \n",
    "    for pile in range(3):\n",
    "        s = st[pile] ^ xored\n",
    "        \n",
    "        if s <= st[pile]:\n",
    "            return st[pile]-s, pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91bb2b64-674d-4073-9cb8-8e8ddb480faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_qlearner(_st, random_chance=None):\n",
    "    \"\"\"\n",
    "    _st: game state [a, b, c]\n",
    "    random_chance: if none, then just play as player. If some number [0, 1], then\n",
    "        this is the probability of making a random move instead\n",
    "    \"\"\"\n",
    "    if random_chance is not None:\n",
    "        if random() <= random_chance:\n",
    "            return nim_random(_st)\n",
    "    global qtable\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    return move, pile  # action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c254ac4-c97c-47fe-b1d7-22d807ade2b6",
   "metadata": {},
   "source": [
    "### Train QLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e41456a-8917-4754-bfbb-096b003402cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable, Alpha, Gamma, Reward = None, 1.0, 0.8, 100.0\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n):\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in tqdm(range(_n)):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0)  # I won\n",
    "                break  # new game\n",
    "\n",
    "            elif np.max(qtable[st2[0], st2[1], st2[2]]) >= Reward:\n",
    "                # immediate loss - penalize it\n",
    "                qtable_update(-Reward, st1, move, pile, np.min(qtable[st2[0], st2[1], st2[2]]))\n",
    "\n",
    "            else:\n",
    "                # not immediate loss - reward it\n",
    "                qtable_update(Reward, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            # Switch sides for play and learning\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e41e814-b1ad-4de8-aaf0-08dfbba57f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3588f9ed6bd1442f8240e60c308152a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nim_qlearn(300000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56963d9c-1fd2-443b-a160-aee39a23c0ad",
   "metadata": {},
   "source": [
    "## 4. Generate NIM Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d710810d-6049-4310-bb8e-d50a1edc13e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PILE = {0:'a', 1:'b', 2:'c'}\n",
    "\n",
    "def save_move(fn, state, move, pile, side):\n",
    "    dir_path = os.path.dirname(os.path.abspath(fn))\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    with open(fn, 'a') as fout:\n",
    "        fout.write(f'a{state[0]}/b{state[1]}/c{state[2]} {side} - {PILE[pile]}{move}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c387343-3e1c-4d14-bb35-1047fb434316",
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(a, b, fn=None, random_chance=None):\n",
    "    state, side = init_game(), a[0]\n",
    "    while True:\n",
    "        engine = Engines[a] if side == a[0] else Engines[b]\n",
    "        move, pile = engine(state, random_chance=random_chance)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        if fn is not None:\n",
    "            save_move(fn, state, move, pile, side)\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "        side = b[0] if side == a[0] else a[0]  # switch sides\n",
    "\n",
    "def play_games(_n, a, b, fn=None, random_chance=None):\n",
    "    \"\"\"\n",
    "    _n: number of games\n",
    "    a: player A function\n",
    "    b: player B function\n",
    "    fn: filename to save moves\n",
    "    random_chance: random chance to choose random move (none is no random)\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for i in range(_n):\n",
    "        wins[game(a, b, fn=fn, random_chance=random_chance)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {a:>8s}{wins[a[0]]:5d}  {b:>8s}{wins[b[0]]:5d}\")\n",
    "    \n",
    "    return wins[a[0]], wins[b[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b908926a-ceb6-4196-86ad-75c8ac8683e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games,     Guru  763    Random  237\n",
      "1000 games,     Guru  928  Qlearner   72\n",
      "1000 games, Qlearner  942      Guru   58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(942, 58)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_games(1000, 'Guru', 'Random', fn=\"test.txt\", random_chance=0.5)\n",
    "play_games(1000, 'Guru', 'Qlearner')\n",
    "play_games(1000, 'Qlearner', 'Guru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef1fc3da-c409-46bd-a380-0eefcc599e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Random', 'Guru'), ('Random', 'Qlearner'), ('Guru', 'Random'), ('Guru', 'Qlearner'), ('Qlearner', 'Random'), ('Qlearner', 'Guru')]\n",
      "['/home/michael/Workspace/nlp-chess/src/bert/nim_datasets/0_prcnt_random', '/home/michael/Workspace/nlp-chess/src/bert/nim_datasets/10_prcnt_random', '/home/michael/Workspace/nlp-chess/src/bert/nim_datasets/20_prcnt_random', '/home/michael/Workspace/nlp-chess/src/bert/nim_datasets/30_prcnt_random', '/home/michael/Workspace/nlp-chess/src/bert/nim_datasets/40_prcnt_random', '/home/michael/Workspace/nlp-chess/src/bert/nim_datasets/50_prcnt_random', '/home/michael/Workspace/nlp-chess/src/bert/nim_datasets/60_prcnt_random', '/home/michael/Workspace/nlp-chess/src/bert/nim_datasets/70_prcnt_random', '/home/michael/Workspace/nlp-chess/src/bert/nim_datasets/80_prcnt_random', '/home/michael/Workspace/nlp-chess/src/bert/nim_datasets/90_prcnt_random', '/home/michael/Workspace/nlp-chess/src/bert/nim_datasets/100_prcnt_random']\n"
     ]
    }
   ],
   "source": [
    "def generate_roster(player_names):\n",
    "    res = []\n",
    "\n",
    "    for comb in itertools.permutations(player_names, 2):\n",
    "        res.append(comb)\n",
    "    return res\n",
    "\n",
    "roster = generate_roster(list(Engines.keys()))\n",
    "print(roster)\n",
    "\n",
    "chances = [num for num in range(0, 11, 1)]\n",
    "\n",
    "root_path = os.path.abspath(\"./nim_datasets\")\n",
    "\n",
    "chance_paths = [os.path.join(root_path, str(chance * 10) + \"_prcnt_random\")\n",
    "                             for chance in chances]\n",
    "print(chance_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f842ce9-5dc4-4224-8221-4b99de034848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "200 games,   Random    0      Guru  200\n",
      "200 games,   Random    1  Qlearner  199\n",
      "200 games,     Guru  200    Random    0\n",
      "200 games,     Guru  188  Qlearner   12\n",
      "200 games, Qlearner  200    Random    0\n",
      "200 games, Qlearner  186      Guru   14\n",
      "0.1\n",
      "200 games,   Random   10      Guru  190\n",
      "200 games,   Random   16  Qlearner  184\n",
      "200 games,     Guru  192    Random    8\n",
      "200 games,     Guru  153  Qlearner   47\n",
      "200 games, Qlearner  190    Random   10\n",
      "200 games, Qlearner  151      Guru   49\n",
      "0.2\n",
      "200 games,   Random   24      Guru  176\n",
      "200 games,   Random   22  Qlearner  178\n",
      "200 games,     Guru  186    Random   14\n",
      "200 games,     Guru  135  Qlearner   65\n",
      "200 games, Qlearner  181    Random   19\n",
      "200 games, Qlearner  138      Guru   62\n",
      "0.3\n",
      "200 games,   Random   34      Guru  166\n",
      "200 games,   Random   34  Qlearner  166\n",
      "200 games,     Guru  176    Random   24\n",
      "200 games,     Guru  120  Qlearner   80\n",
      "200 games, Qlearner  175    Random   25\n",
      "200 games, Qlearner  103      Guru   97\n",
      "0.4\n",
      "200 games,   Random   28      Guru  172\n",
      "200 games,   Random   43  Qlearner  157\n",
      "200 games,     Guru  166    Random   34\n",
      "200 games,     Guru   97  Qlearner  103\n",
      "200 games, Qlearner  156    Random   44\n",
      "200 games, Qlearner  111      Guru   89\n",
      "0.5\n",
      "200 games,   Random   64      Guru  136\n",
      "200 games,   Random   53  Qlearner  147\n",
      "200 games,     Guru  146    Random   54\n",
      "200 games,     Guru  103  Qlearner   97\n",
      "200 games, Qlearner  153    Random   47\n",
      "200 games, Qlearner   91      Guru  109\n",
      "0.6\n",
      "200 games,   Random   47      Guru  153\n",
      "200 games,   Random   58  Qlearner  142\n",
      "200 games,     Guru  156    Random   44\n",
      "200 games,     Guru  103  Qlearner   97\n",
      "200 games, Qlearner  147    Random   53\n",
      "200 games, Qlearner   98      Guru  102\n",
      "0.7\n",
      "200 games,   Random   65      Guru  135\n",
      "200 games,   Random   70  Qlearner  130\n",
      "200 games,     Guru  146    Random   54\n",
      "200 games,     Guru  101  Qlearner   99\n",
      "200 games, Qlearner  131    Random   69\n",
      "200 games, Qlearner   99      Guru  101\n",
      "0.8\n",
      "200 games,   Random   68      Guru  132\n",
      "200 games,   Random   94  Qlearner  106\n",
      "200 games,     Guru  124    Random   76\n",
      "200 games,     Guru  102  Qlearner   98\n",
      "200 games, Qlearner  127    Random   73\n",
      "200 games, Qlearner   95      Guru  105\n",
      "0.9\n",
      "200 games,   Random   85      Guru  115\n",
      "200 games,   Random   93  Qlearner  107\n",
      "200 games,     Guru  119    Random   81\n",
      "200 games,     Guru  109  Qlearner   91\n",
      "200 games, Qlearner  115    Random   85\n",
      "200 games, Qlearner  102      Guru   98\n",
      "1.0\n",
      "200 games,   Random   97      Guru  103\n",
      "200 games,   Random  105  Qlearner   95\n",
      "200 games,     Guru  103    Random   97\n",
      "200 games,     Guru   93  Qlearner  107\n",
      "200 games, Qlearner   99    Random  101\n",
      "200 games, Qlearner  103      Guru   97\n"
     ]
    }
   ],
   "source": [
    "flat_datasets = []\n",
    "dict_datasets = {}\n",
    "\n",
    "for chance in chances:\n",
    "    \n",
    "    chance = chance / 10.0\n",
    "    str_chance = str(int(chance * 100))\n",
    "    print(chance)\n",
    "    \n",
    "    dict_datasets[str_chance] = []\n",
    "    \n",
    "    # Upper directory for datasets of this random chance\n",
    "    chance_dir = os.path.join(root_path, str_chance + \"_prcnt_random\")\n",
    "    \n",
    "    for matchup in roster:\n",
    "        dataset_path = os.path.join(chance_dir, \n",
    "                                    f\"{matchup[0]}_{matchup[1]}.txt\")\n",
    "        \n",
    "        flat_datasets.append(dataset_path)\n",
    "        dict_datasets[str_chance].append(dataset_path)\n",
    "        \n",
    "        play_games(200, *matchup, random_chance=chance, fn=dataset_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c6e9c7f-8c06-4769-b813-9c53de2c11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nim_datasets = flat_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b9dfab-1ea9-40b3-aa5d-8fa940fca932",
   "metadata": {},
   "source": [
    "## 5. Train the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b84df26-716d-4807-b898-d38096f90f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'c2': 40, '##4': 31, 'b2': 41, '1': 8, 'c0': 37, '-': 5, '7': 14, 'c': 22, 'a8': 57, 'b3': 44, 'a1': 33, '3': 10, '[SEP]': 3, 'a9': 62, 'G': 17, 'Q': 18, 'b4': 45, '##0': 23, '##3': 28, '/': 6, 'b7': 55, 'c8': 59, 'c10': 65, 'b8': 58, 'b9': 61, '2': 9, 'R': 19, '[PAD]': 0, 'a5': 50, 'a0': 36, 'a': 20, '[MASK]': 4, '5': 12, '6': 13, '0': 7, '##1': 24, '##2': 29, '9': 16, '[UNK]': 1, 'a2': 39, 'b5': 48, 'a6': 53, 'c7': 54, '##8': 32, '##9': 26, 'c3': 43, '4': 11, 'c9': 60, 'b0': 38, 'a10': 63, 'a7': 56, 'c1': 34, '##6': 25, 'b1': 35, 'c4': 46, 'b10': 64, 'b6': 52, 'c6': 51, 'a3': 42, 'c5': 49, '[CLS]': 2, '##5': 30, 'a4': 47, '##7': 27, 'b': 21, '8': 15}\n",
      "\n",
      "\n",
      "\n",
      "Saved tokenizer to /home/michael/Workspace/nlp-chess/src/bert/nim-tokenizer/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=False, clean_text=True)\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=nim_datasets, vocab_size=MAX_VOCAB, min_frequency=1,\n",
    "                show_progress=True)\n",
    "\n",
    "print(tokenizer.get_vocab())\n",
    "\n",
    "save_path = tokenizer.save_model(TOKENIZER_DIR)\n",
    "print(f\"\\nSaved tokenizer to {save_path[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0318011b-e189-49e7-84a7-08dcb4617928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Workspace/nlp-chess/env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1646: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Reimport the tokenizer to test it was successful\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH, max_len=MAX_STR_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60cada-4bdb-47a2-842b-378357402a99",
   "metadata": {},
   "source": [
    "## 6. Build the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3598710-9a9d-493f-abad-51055c63940a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-09d29cfe395c19c2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/michael/.cache/huggingface/datasets/text/default-09d29cfe395c19c2/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a012c62e4e41da9df5a3bc916087f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993779c68c0f4ae3b6d56f8875121fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/michael/.cache/huggingface/datasets/text/default-09d29cfe395c19c2/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Using custom data configuration default-bf4d44678a418644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/michael/.cache/huggingface/datasets/text/default-bf4d44678a418644/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3187b5d72ea2412f85cd26ba5d28a6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a24bb40b7a548bb8daf1c98f8216f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/michael/.cache/huggingface/datasets/text/default-bf4d44678a418644/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Using custom data configuration default-03a10f9abab416ff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/michael/.cache/huggingface/datasets/text/default-03a10f9abab416ff/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9688b93bf3e04bd59861e090beecae97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0057a1b8d24f0ab724b25af715db23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/michael/.cache/huggingface/datasets/text/default-03a10f9abab416ff/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Using custom data configuration default-5dce8196b210d771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/michael/.cache/huggingface/datasets/text/default-5dce8196b210d771/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de4a7dca3384671bbf66134fed886b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d804e367b0472aba09ec86998f45fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/michael/.cache/huggingface/datasets/text/default-5dce8196b210d771/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Using custom data configuration default-d7e829a21d8a0b2e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/michael/.cache/huggingface/datasets/text/default-d7e829a21d8a0b2e/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9c3684020240fabcb3fded8091d95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9da40975f14ac79ed73ceddcd22dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/michael/.cache/huggingface/datasets/text/default-d7e829a21d8a0b2e/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Using custom data configuration default-598359f1e7706b2e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/michael/.cache/huggingface/datasets/text/default-598359f1e7706b2e/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f7209641aa41f7a0a01e124b5aedfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468823b99e834674a8b756204ddec3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/michael/.cache/huggingface/datasets/text/default-598359f1e7706b2e/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Using custom data configuration default-83adbbc97844cf75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/michael/.cache/huggingface/datasets/text/default-83adbbc97844cf75/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb57e2126a304017b3c60cf5f2a95eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcde78c305a49548fba76795185b438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/michael/.cache/huggingface/datasets/text/default-83adbbc97844cf75/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Using custom data configuration default-106389a32593c1f3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/michael/.cache/huggingface/datasets/text/default-106389a32593c1f3/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5979e131db46c897584f43794691cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4febdef9cd3c41e781effc12e5243e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/michael/.cache/huggingface/datasets/text/default-106389a32593c1f3/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Using custom data configuration default-3c9861c4bdbcee61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/michael/.cache/huggingface/datasets/text/default-3c9861c4bdbcee61/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750135050e614a9cbd6739ef3d1cda2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21777c2e7eb14c8b9e66e08599e5ddc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/michael/.cache/huggingface/datasets/text/default-3c9861c4bdbcee61/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Using custom data configuration default-3977c1bf37cb70c8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/michael/.cache/huggingface/datasets/text/default-3977c1bf37cb70c8/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98fb6fd597ca444ead7ab6eeae3d0184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d2a79b28b543ebbf8773e869dc36e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/michael/.cache/huggingface/datasets/text/default-3977c1bf37cb70c8/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Using custom data configuration default-590f9839f48166db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/michael/.cache/huggingface/datasets/text/default-590f9839f48166db/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be29fdc34de40fda7bf368c67cc21c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e697d46a531041ef82e12e4abc5d23c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/michael/.cache/huggingface/datasets/text/default-590f9839f48166db/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_dataset(path):\n",
    "\n",
    "    raw_datasets = load_dataset('text', data_files=path,\n",
    "                                split='train')\n",
    "\n",
    "    #cut size in half\n",
    "    raw_datasets = raw_datasets.shuffle(seed=42)\n",
    "\n",
    "    #raw_datasets = raw_datasets.select(range(10000))\n",
    "    raw_datasets = raw_datasets.train_test_split()\n",
    "\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding='max_length', truncation=True)\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function, \n",
    "                                          batched=BATCHED, \n",
    "                                          keep_in_memory=True, \n",
    "                                          num_proc=NUM_PROC, \n",
    "                                          remove_columns=[\"text\"])\n",
    "    \n",
    "    return tokenized_datasets\n",
    "\n",
    "tokenized_datasets = {key: tokenize_dataset(dict_datasets[key]) for key in dict_datasets.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daa58cf7-c97f-42f9-9319-6b9b61e6ca45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       " 'input_ids': [2, 39, 6, 38, 6, 37, 1, 5, 39, 3, 0, 0],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"0\"][\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "817f00c2-0ee1-41f4-ac32-d016c7681836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    }
   ],
   "source": [
    "block_size = MAX_STR_LEN\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    #result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "datasets = {key: tokenized_datasets[key].map(group_texts,\n",
    "                batched=BATCHED,\n",
    "                batch_size=1000,\n",
    "                num_proc=NUM_PROC,\n",
    "                keep_in_memory=True)\n",
    "            for key in tokenized_datasets.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03fa41-2db5-49a4-9da6-d811b23c251b",
   "metadata": {},
   "source": [
    "## 7. Build the BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f798d03-82b2-4895-9026-9caa9077af68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  14830914\n"
     ]
    }
   ],
   "source": [
    "# Set a configuration for our model\n",
    "config = BertConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    max_position_embeddings=MAX_STR_LEN,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=NUM_HIDDEN_LAYERS\n",
    ")\n",
    "# Initialize the model from a configuration without pretrained weights\n",
    "model = BertForMaskedLM(config=config)\n",
    "print('Num parameters: ',model.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ba736-e0b1-4af8-ab8b-54860f863cf3",
   "metadata": {},
   "source": [
    "## 8. Train the BERT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39ba2c17-6330-4bfb-87ea-afd3f9e750d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=3200,\n",
    "    output_dir='./output-nim', \n",
    "    num_train_epochs=1600,\n",
    "    eval_steps=400,\n",
    "    evaluation_strategy=\"steps\")\n",
    "\n",
    "def do_training(save_name, train_dataset, eval_dataset):\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm_probability=0.15,\n",
    "    )\n",
    "    \n",
    "    model = BertForMaskedLM(config=config)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=full_train_dataset, \n",
    "        eval_dataset=full_eval_dataset, \n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    trainer.save_model(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e648e497-6108-40c3-97ce-96fd97eb30fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Workspace/nlp-chess/env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 6041\n",
      "  Num Epochs = 1600\n",
      "  Instantaneous batch size per device = 3200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 20:07, Epoch 1600/1600]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.894379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.004600</td>\n",
       "      <td>0.825257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.788200</td>\n",
       "      <td>0.751511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.728200</td>\n",
       "      <td>0.721265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.693200</td>\n",
       "      <td>0.744999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.693200</td>\n",
       "      <td>0.716694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.667900</td>\n",
       "      <td>0.774669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.657400</td>\n",
       "      <td>0.702898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2014\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-500\n",
      "Configuration saved in ./output-nim/checkpoint-500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2014\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1000\n",
      "Configuration saved in ./output-nim/checkpoint-1000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2014\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1500\n",
      "Configuration saved in ./output-nim/checkpoint-1500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2014\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2014\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2000\n",
      "Configuration saved in ./output-nim/checkpoint-2000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2014\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2500\n",
      "Configuration saved in ./output-nim/checkpoint-2500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2014\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-3000\n",
      "Configuration saved in ./output-nim/checkpoint-3000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2014\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./0-nim\n",
      "Configuration saved in ./0-nim/config.json\n",
      "Model weights saved in ./0-nim/pytorch_model.bin\n",
      "/home/michael/Workspace/nlp-chess/env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5934\n",
      "  Num Epochs = 1600\n",
      "  Instantaneous batch size per device = 3200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 19:37, Epoch 1600/1600]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.919479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.008900</td>\n",
       "      <td>0.832182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.850327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.766200</td>\n",
       "      <td>0.800778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.813228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.855394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.718400</td>\n",
       "      <td>0.827184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.705200</td>\n",
       "      <td>0.756895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1978\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-500\n",
      "Configuration saved in ./output-nim/checkpoint-500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1978\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1000\n",
      "Configuration saved in ./output-nim/checkpoint-1000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1978\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1500\n",
      "Configuration saved in ./output-nim/checkpoint-1500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1978\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1978\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2000\n",
      "Configuration saved in ./output-nim/checkpoint-2000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1978\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2500\n",
      "Configuration saved in ./output-nim/checkpoint-2500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1978\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-3000\n",
      "Configuration saved in ./output-nim/checkpoint-3000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1978\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./10-nim\n",
      "Configuration saved in ./10-nim/config.json\n",
      "Model weights saved in ./10-nim/pytorch_model.bin\n",
      "/home/michael/Workspace/nlp-chess/env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5947\n",
      "  Num Epochs = 1600\n",
      "  Instantaneous batch size per device = 3200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 19:39, Epoch 1600/1600]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.935952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.032800</td>\n",
       "      <td>0.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.864800</td>\n",
       "      <td>0.848879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.815800</td>\n",
       "      <td>0.903867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.787100</td>\n",
       "      <td>0.834511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.787100</td>\n",
       "      <td>0.870851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.769200</td>\n",
       "      <td>0.888708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.753500</td>\n",
       "      <td>0.843373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-500\n",
      "Configuration saved in ./output-nim/checkpoint-500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1000\n",
      "Configuration saved in ./output-nim/checkpoint-1000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1500\n",
      "Configuration saved in ./output-nim/checkpoint-1500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2000\n",
      "Configuration saved in ./output-nim/checkpoint-2000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2500\n",
      "Configuration saved in ./output-nim/checkpoint-2500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-3000\n",
      "Configuration saved in ./output-nim/checkpoint-3000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1983\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./20-nim\n",
      "Configuration saved in ./20-nim/config.json\n",
      "Model weights saved in ./20-nim/pytorch_model.bin\n",
      "/home/michael/Workspace/nlp-chess/env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5848\n",
      "  Num Epochs = 1600\n",
      "  Instantaneous batch size per device = 3200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 19:20, Epoch 1600/1600]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.957375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.038600</td>\n",
       "      <td>0.909758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.889400</td>\n",
       "      <td>0.911581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.850500</td>\n",
       "      <td>0.952642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.822500</td>\n",
       "      <td>0.888505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.822500</td>\n",
       "      <td>0.888008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.802900</td>\n",
       "      <td>0.887126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.789200</td>\n",
       "      <td>0.877340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1950\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-500\n",
      "Configuration saved in ./output-nim/checkpoint-500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1950\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1000\n",
      "Configuration saved in ./output-nim/checkpoint-1000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1950\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1500\n",
      "Configuration saved in ./output-nim/checkpoint-1500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1950\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1950\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2000\n",
      "Configuration saved in ./output-nim/checkpoint-2000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1950\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2500\n",
      "Configuration saved in ./output-nim/checkpoint-2500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1950\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-3000\n",
      "Configuration saved in ./output-nim/checkpoint-3000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1950\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./30-nim\n",
      "Configuration saved in ./30-nim/config.json\n",
      "Model weights saved in ./30-nim/pytorch_model.bin\n",
      "/home/michael/Workspace/nlp-chess/env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5877\n",
      "  Num Epochs = 1600\n",
      "  Instantaneous batch size per device = 3200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 19:24, Epoch 1600/1600]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.972604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.051300</td>\n",
       "      <td>0.905123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.915600</td>\n",
       "      <td>0.944507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.878800</td>\n",
       "      <td>0.955856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.853300</td>\n",
       "      <td>0.981495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.853300</td>\n",
       "      <td>1.003121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.835900</td>\n",
       "      <td>0.971576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.823700</td>\n",
       "      <td>0.914322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1960\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-500\n",
      "Configuration saved in ./output-nim/checkpoint-500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1960\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1000\n",
      "Configuration saved in ./output-nim/checkpoint-1000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1960\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1500\n",
      "Configuration saved in ./output-nim/checkpoint-1500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1960\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1960\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2000\n",
      "Configuration saved in ./output-nim/checkpoint-2000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1960\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2500\n",
      "Configuration saved in ./output-nim/checkpoint-2500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1960\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-3000\n",
      "Configuration saved in ./output-nim/checkpoint-3000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1960\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./40-nim\n",
      "Configuration saved in ./40-nim/config.json\n",
      "Model weights saved in ./40-nim/pytorch_model.bin\n",
      "/home/michael/Workspace/nlp-chess/env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5886\n",
      "  Num Epochs = 1600\n",
      "  Instantaneous batch size per device = 3200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 19:28, Epoch 1600/1600]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.989297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.073100</td>\n",
       "      <td>1.010626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.938800</td>\n",
       "      <td>0.997403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.908100</td>\n",
       "      <td>0.980984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.885700</td>\n",
       "      <td>0.919443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.885700</td>\n",
       "      <td>0.971414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.870300</td>\n",
       "      <td>1.006132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.860300</td>\n",
       "      <td>0.975694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-500\n",
      "Configuration saved in ./output-nim/checkpoint-500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1000\n",
      "Configuration saved in ./output-nim/checkpoint-1000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1500\n",
      "Configuration saved in ./output-nim/checkpoint-1500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2000\n",
      "Configuration saved in ./output-nim/checkpoint-2000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2500\n",
      "Configuration saved in ./output-nim/checkpoint-2500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-3000\n",
      "Configuration saved in ./output-nim/checkpoint-3000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./50-nim\n",
      "Configuration saved in ./50-nim/config.json\n",
      "Model weights saved in ./50-nim/pytorch_model.bin\n",
      "/home/michael/Workspace/nlp-chess/env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5843\n",
      "  Num Epochs = 1600\n",
      "  Instantaneous batch size per device = 3200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 19:17, Epoch 1600/1600]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.990865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.071200</td>\n",
       "      <td>0.953962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.958829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.927200</td>\n",
       "      <td>1.030709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.908400</td>\n",
       "      <td>1.026310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.908400</td>\n",
       "      <td>0.974243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.894400</td>\n",
       "      <td>1.017209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>1.043193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1948\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-500\n",
      "Configuration saved in ./output-nim/checkpoint-500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1948\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1000\n",
      "Configuration saved in ./output-nim/checkpoint-1000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1948\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1500\n",
      "Configuration saved in ./output-nim/checkpoint-1500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1948\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1948\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2000\n",
      "Configuration saved in ./output-nim/checkpoint-2000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1948\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2500\n",
      "Configuration saved in ./output-nim/checkpoint-2500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1948\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-3000\n",
      "Configuration saved in ./output-nim/checkpoint-3000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1948\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./60-nim\n",
      "Configuration saved in ./60-nim/config.json\n",
      "Model weights saved in ./60-nim/pytorch_model.bin\n",
      "/home/michael/Workspace/nlp-chess/env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5875\n",
      "  Num Epochs = 1600\n",
      "  Instantaneous batch size per device = 3200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 19:24, Epoch 1600/1600]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.959303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.081900</td>\n",
       "      <td>1.006147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>0.997497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.943200</td>\n",
       "      <td>0.989638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.925600</td>\n",
       "      <td>0.990977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.925600</td>\n",
       "      <td>1.039234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.915100</td>\n",
       "      <td>1.014785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.905500</td>\n",
       "      <td>1.060272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1959\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-500\n",
      "Configuration saved in ./output-nim/checkpoint-500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1959\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1000\n",
      "Configuration saved in ./output-nim/checkpoint-1000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1959\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1500\n",
      "Configuration saved in ./output-nim/checkpoint-1500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1959\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1959\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2000\n",
      "Configuration saved in ./output-nim/checkpoint-2000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1959\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2500\n",
      "Configuration saved in ./output-nim/checkpoint-2500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1959\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-3000\n",
      "Configuration saved in ./output-nim/checkpoint-3000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1959\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./70-nim\n",
      "Configuration saved in ./70-nim/config.json\n",
      "Model weights saved in ./70-nim/pytorch_model.bin\n",
      "/home/michael/Workspace/nlp-chess/env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5884\n",
      "  Num Epochs = 1600\n",
      "  Instantaneous batch size per device = 3200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 19:25, Epoch 1600/1600]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.013289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.072400</td>\n",
       "      <td>1.003014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.964400</td>\n",
       "      <td>1.038501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.946100</td>\n",
       "      <td>1.011220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.931000</td>\n",
       "      <td>0.983576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.931000</td>\n",
       "      <td>0.969384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.918600</td>\n",
       "      <td>1.045261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.910300</td>\n",
       "      <td>1.009902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-500\n",
      "Configuration saved in ./output-nim/checkpoint-500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1000\n",
      "Configuration saved in ./output-nim/checkpoint-1000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1500\n",
      "Configuration saved in ./output-nim/checkpoint-1500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2000\n",
      "Configuration saved in ./output-nim/checkpoint-2000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2500\n",
      "Configuration saved in ./output-nim/checkpoint-2500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-3000\n",
      "Configuration saved in ./output-nim/checkpoint-3000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1962\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./80-nim\n",
      "Configuration saved in ./80-nim/config.json\n",
      "Model weights saved in ./80-nim/pytorch_model.bin\n",
      "/home/michael/Workspace/nlp-chess/env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5907\n",
      "  Num Epochs = 1600\n",
      "  Instantaneous batch size per device = 3200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 19:31, Epoch 1600/1600]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.987094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.073600</td>\n",
       "      <td>1.053536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.997236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.945600</td>\n",
       "      <td>0.986522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.930600</td>\n",
       "      <td>1.015485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.930600</td>\n",
       "      <td>1.017046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>1.031293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.912800</td>\n",
       "      <td>0.969582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1970\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-500\n",
      "Configuration saved in ./output-nim/checkpoint-500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1970\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1000\n",
      "Configuration saved in ./output-nim/checkpoint-1000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1970\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1500\n",
      "Configuration saved in ./output-nim/checkpoint-1500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1970\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1970\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2000\n",
      "Configuration saved in ./output-nim/checkpoint-2000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1970\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2500\n",
      "Configuration saved in ./output-nim/checkpoint-2500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1970\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-3000\n",
      "Configuration saved in ./output-nim/checkpoint-3000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1970\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./90-nim\n",
      "Configuration saved in ./90-nim/config.json\n",
      "Model weights saved in ./90-nim/pytorch_model.bin\n",
      "/home/michael/Workspace/nlp-chess/env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5931\n",
      "  Num Epochs = 1600\n",
      "  Instantaneous batch size per device = 3200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 19:38, Epoch 1600/1600]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.000779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.072100</td>\n",
       "      <td>0.988808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.961700</td>\n",
       "      <td>0.984789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.945900</td>\n",
       "      <td>1.000607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>0.988148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>1.021623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.922100</td>\n",
       "      <td>0.990636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.912800</td>\n",
       "      <td>1.005840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1977\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-500\n",
      "Configuration saved in ./output-nim/checkpoint-500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1977\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1000\n",
      "Configuration saved in ./output-nim/checkpoint-1000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1977\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-1500\n",
      "Configuration saved in ./output-nim/checkpoint-1500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1977\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1977\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2000\n",
      "Configuration saved in ./output-nim/checkpoint-2000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1977\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-2500\n",
      "Configuration saved in ./output-nim/checkpoint-2500/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1977\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output-nim/checkpoint-3000\n",
      "Configuration saved in ./output-nim/checkpoint-3000/config.json\n",
      "Model weights saved in ./output-nim/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1977\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./100-nim\n",
      "Configuration saved in ./100-nim/config.json\n",
      "Model weights saved in ./100-nim/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "for key in tokenized_datasets.keys():\n",
    "    \n",
    "\n",
    "    full_train_dataset = tokenized_datasets[key][\"train\"]\n",
    "    full_eval_dataset = tokenized_datasets[key][\"test\"]\n",
    "\n",
    "    do_training(f\"./{key}-nim\", full_train_dataset, full_eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74f9e8-90d4-4472-b4b0-d3cd260df2b0",
   "metadata": {},
   "source": [
    "## 9. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75387d7e-7be9-45a9-b0f2-215b631ce5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./0-nim/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./0-nim\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 12,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 66\n",
      "}\n",
      "\n",
      "loading weights file ./0-nim/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at ./0-nim.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Didn't find file ./nim-tokenizer/added_tokens.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer.json. We won't load it.\n",
      "loading file ./nim-tokenizer/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "file ./nim-tokenizer/config.json not found\n",
      "loading configuration file ./10-nim/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./10-nim\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 12,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 66\n",
      "}\n",
      "\n",
      "loading weights file ./10-nim/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at ./10-nim.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Didn't find file ./nim-tokenizer/added_tokens.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer.json. We won't load it.\n",
      "loading file ./nim-tokenizer/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "file ./nim-tokenizer/config.json not found\n",
      "loading configuration file ./20-nim/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./20-nim\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 12,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 66\n",
      "}\n",
      "\n",
      "loading weights file ./20-nim/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at ./20-nim.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Didn't find file ./nim-tokenizer/added_tokens.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer.json. We won't load it.\n",
      "loading file ./nim-tokenizer/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "file ./nim-tokenizer/config.json not found\n",
      "loading configuration file ./30-nim/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./30-nim\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 12,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 66\n",
      "}\n",
      "\n",
      "loading weights file ./30-nim/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at ./30-nim.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Didn't find file ./nim-tokenizer/added_tokens.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer.json. We won't load it.\n",
      "loading file ./nim-tokenizer/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "file ./nim-tokenizer/config.json not found\n",
      "loading configuration file ./40-nim/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./40-nim\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 12,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 66\n",
      "}\n",
      "\n",
      "loading weights file ./40-nim/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at ./40-nim.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Didn't find file ./nim-tokenizer/added_tokens.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer.json. We won't load it.\n",
      "loading file ./nim-tokenizer/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "file ./nim-tokenizer/config.json not found\n",
      "loading configuration file ./50-nim/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./50-nim\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 12,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 66\n",
      "}\n",
      "\n",
      "loading weights file ./50-nim/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at ./50-nim.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Didn't find file ./nim-tokenizer/added_tokens.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer.json. We won't load it.\n",
      "loading file ./nim-tokenizer/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "file ./nim-tokenizer/config.json not found\n",
      "loading configuration file ./60-nim/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./60-nim\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 12,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 66\n",
      "}\n",
      "\n",
      "loading weights file ./60-nim/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at ./60-nim.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Didn't find file ./nim-tokenizer/added_tokens.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer.json. We won't load it.\n",
      "loading file ./nim-tokenizer/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "file ./nim-tokenizer/config.json not found\n",
      "loading configuration file ./70-nim/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./70-nim\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 12,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 66\n",
      "}\n",
      "\n",
      "loading weights file ./70-nim/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at ./70-nim.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Didn't find file ./nim-tokenizer/added_tokens.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer.json. We won't load it.\n",
      "loading file ./nim-tokenizer/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "file ./nim-tokenizer/config.json not found\n",
      "loading configuration file ./80-nim/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./80-nim\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 12,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 66\n",
      "}\n",
      "\n",
      "loading weights file ./80-nim/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at ./80-nim.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Didn't find file ./nim-tokenizer/added_tokens.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer.json. We won't load it.\n",
      "loading file ./nim-tokenizer/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "file ./nim-tokenizer/config.json not found\n",
      "loading configuration file ./90-nim/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./90-nim\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 12,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 66\n",
      "}\n",
      "\n",
      "loading weights file ./90-nim/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at ./90-nim.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Didn't find file ./nim-tokenizer/added_tokens.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer.json. We won't load it.\n",
      "loading file ./nim-tokenizer/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "file ./nim-tokenizer/config.json not found\n",
      "loading configuration file ./100-nim/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./100-nim\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 12,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 66\n",
      "}\n",
      "\n",
      "loading weights file ./100-nim/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at ./100-nim.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Didn't find file ./nim-tokenizer/added_tokens.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./nim-tokenizer/tokenizer.json. We won't load it.\n",
      "loading file ./nim-tokenizer/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "file ./nim-tokenizer/config.json not found\n"
     ]
    }
   ],
   "source": [
    "models = {key: BertHarmon(f\"./{key}-nim\", \"./nim-tokenizer\")\n",
    "          for key in tokenized_datasets.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4a43fff-699a-4881-b3cc-4441e5bb5b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Nim player\n",
    "def nim_bert(_st, random_chance=None):\n",
    "    lk = {'a':0, 'b':1, 'c':2}\n",
    "    move = model.pipeline(f\"a{_st[0]}/b{_st[1]}/c{_st[2]}\" + \" Q - [MASK]\")[0]\n",
    "    try:\n",
    "        r = move[\"token_str\"]\n",
    "        r = r.replace(\" \",\"\")\n",
    "        \n",
    "        return int(r[1:]), lk[r[0]]\n",
    "        \n",
    "    except:\n",
    "        print(f'impos bert move {move}, returning random move')\n",
    "        return nim_random(_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "168d0840-f6dd-4500-a433-b63515e6523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines[\"BERT\"] = nim_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28be61-0c71-497d-9b61-87b8294cc6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200 games,   Random    0      Guru  200\n",
      "200 games,   Random    0  Qlearner  200\n",
      "200 games,   Random    6      BERT  194\n",
      "200 games,     Guru  200    Random    0\n",
      "200 games,     Guru  190  Qlearner   10\n",
      "200 games,     Guru  194      BERT    6\n",
      "200 games, Qlearner  197    Random    3\n",
      "200 games, Qlearner  186      Guru   14\n",
      "200 games, Qlearner  187      BERT   13\n",
      "200 games,     BERT  195    Random    5\n",
      "200 games,     BERT   75      Guru  125\n",
      "200 games,     BERT   76  Qlearner  124\n",
      "10\n",
      "200 games,   Random   13      Guru  187\n",
      "200 games,   Random    8  Qlearner  192\n",
      "200 games,   Random   26      BERT  174\n",
      "200 games,     Guru  189    Random   11\n",
      "200 games,     Guru  154  Qlearner   46\n",
      "200 games,     Guru  147      BERT   53\n",
      "200 games, Qlearner  191    Random    9\n",
      "200 games, Qlearner  148      Guru   52\n",
      "200 games, Qlearner  147      BERT   53\n",
      "200 games,     BERT  185    Random   15\n",
      "200 games,     BERT   89      Guru  111\n",
      "200 games,     BERT   79  Qlearner  121\n",
      "20\n",
      "200 games,   Random   25      Guru  175\n",
      "200 games,   Random   13  Qlearner  187\n",
      "200 games,   Random    8      BERT  192\n",
      "200 games,     Guru  174    Random   26\n",
      "200 games,     Guru  132  Qlearner   68\n",
      "200 games,     Guru  133      BERT   67\n",
      "200 games, Qlearner  184    Random   16\n",
      "200 games, Qlearner  128      Guru   72\n",
      "200 games, Qlearner  113      BERT   87\n",
      "200 games,     BERT  187    Random   13\n"
     ]
    }
   ],
   "source": [
    "for key in tokenized_datasets.keys():\n",
    "    model = models[key]\n",
    "    \n",
    "    roster = generate_roster(list(Engines.keys()))\n",
    "    print(key)\n",
    "    for matchup in roster:\n",
    "        \n",
    "        play_games(200, *matchup, random_chance=int(key) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576bbec9-b955-452e-b8ca-17d5494cdd93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
